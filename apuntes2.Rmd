---
title: "Resumen del curso: Análisis Avanzado de Datos con Inteligencia Artificial"
author: "David Eduardo Sosa Olea"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    collapsed: true
    smooth_scroll: true
    theme: spacelab
    highlight: tango
    df_print: paged
    code_folding: show
---

# Introducción

## ¿Que es R?

R es un lenguaje de programación y un entorno de software diseñado específicamente para estadísticas y análisis de datos. Desarrollado por estadísticos y científicos computacionales, R proporciona herramientas robustas para manipular, visualizar y modelar datos, convirtiéndolo en una opción popular entre profesionales en campos como la ciencia de datos, la investigación académica y la analítica empresarial. Su naturaleza de código abierto y la amplia gama de paquetes y extensiones disponibles permiten a los usuarios personalizar y ampliar sus capacidades según sus necesidades específicas. R se destaca por su flexibilidad y potencia en la exploración y comprensión de datos, convirtiéndolo en una herramienta valiosa en el mundo del análisis estadístico.

# Importación y Limpieza de Datos

La importación de datos es un proceso crucial en cualquier análisis de datos, ya que la calidad y la precisión de los resultados dependen en gran medida de la calidad de los datos de entrada. La importancia de importar datos desde distintas fuentes radica en la diversidad y complejidad de la información que puede ser crucial para la toma de decisiones informadas

## Fuentes (Excel, CSV)

-   **Excel**: Para importar datos desde archivos Excel, el paquete readxl o openxlsx es comúnmente utilizado. Ejemplo:

```{r, message=FALSE , warning=FALSE, rows.print = 5}

# Especificar la ruta del archivo Excel
ruta_archivo <- "01_Informacion_Data/Bureau_Labor_of_Statistics_Data/Detroit_Warren_Dearborn.xlsx"

```

```{r, message=FALSE , warning=FALSE, rows.print = 5}

# Cargar la biblioteca openxlsx para trabajar con archivos Excel
library(openxlsx)

# Leer datos desde la primera hoja del archivo Excel
Employment_1 <- read.xlsx(ruta_archivo, sheet = 1)

# Imprimir los datos cargados para verificar la lectura correcta del archivo

Employment_1

```

Es frecuente toparse con archivos Excel que presentan encabezados o filas vacías previas a los datos reales. Tanto las bibliotecas readxl como openxlsx brindan una solución sencilla para abordar esta situación.

Aquí tienes un ejemplo utilizando la biblioteca readxl:

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# install.packages("readxl")
library(readxl)

# Leer el archivo Excel, omitiendo 10 filas al principio
Employment <- read_excel(
    path = ruta_archivo,
    sheet = "BLS Data Series",
    skip = 10
)
Employment
```

Este código ejemplifica cómo cargar un archivo Excel, específicamente desde la hoja "BLS Data Series", omitiendo las primeras 10 filas que contienen encabezados o información no deseada.

-   **CSV**: La función read.csv() es usada para importar datos desde archivos CSV en R. Ejemplo:

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Especificar la ruta del archivo CSV
file_ruta <- "01_Informacion_Data/Zillow/Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv"

# Leer el archivo Excel
Zillow <- read.csv(file = file_ruta)

head(Zillow)
```

## Extracción de Datos de Archivos zip

En el ámbito del análisis de datos, la eficiencia y la consistencia en la manipulación de conjuntos de datos son fundamentales. En este contexto, el presente documento explora un script en R diseñado para automatizar la extracción, manipulación y limpieza de datos.

El script aborda la gestión de archivos ZIP que contienen información demográfica clave, extraída de diversas fuentes. A través de una serie de pasos, el código facilita la obtención, procesamiento y consolidación de datos en un formato más manejable y analítico.

El análisis se inicia con la configuración del directorio de trabajo y la identificación de archivos ZIP relevantes. A continuación, se descomprimen estos archivos para revelar conjuntos de datos en formato CSV. Estos datos son entonces leídos, manipulados y agregados en un marco de datos unificado. Además, se realiza una limpieza de archivos temporales para mantener la integridad del espacio de trabajo.

El objetivo de este script es proporcionar una herramienta eficaz para investigadores, analistas y profesionales de datos que deseen analizar y comprender las tendencias demográficas a lo largo del tiempo.

A lo largo del documento, se detallarán cada una de las secciones del código, explicando las decisiones de diseño y brindando orientación sobre cómo adaptar el script a diferentes contextos y conjuntos de datos.

Acompáñenos en este recorrido a través de un proceso automatizado que simplifica la gestión y análisis de datos demográficos del United States Census Bureau en el entorno de programación R.

¡Comencemos!

```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
# Cargar la biblioteca
library(purrr)
library(tidyverse)
library(readr)
```

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Configuración del directorio y archivos ZIP
my_dir <- "01_Informacion_Data/US_Census_Bureau/"
zip_files <- list.files(path = my_dir, pattern = "*.zip", full.names = TRUE)

# Descomprimir archivos ZIP
unzip_files <- map(zip_files, ~unzip(.x, exdir = my_dir))

```

En esta sección, se configura el directorio de trabajo y se identifican los archivos ZIP en el directorio. Luego, se descomprimen los archivos ZIP en la misma ubicación.

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Obtener lista de archivos 
data_Lfiles <- list.files(path = my_dir, full.names = TRUE)
data_Lfiles
```

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Obtener archivos de datos en formato CSV
data_files <- list.files(path = my_dir, pattern = "*Data.csv", full.names = TRUE)

# Leer archivos CSV y almacenar en una lista
pop_data <- map(.x = data_files, .f = read.csv, skip = 1, header = TRUE)

# Agregar la columna 'Year' a cada conjunto de datos
for (i in 1:length(pop_data)) {
    pop_data[[i]]$Year <- 2010 + i
}

# Combinar la lista de conjuntos de datos en un marco de datos
pop_data <- map_dfr(.x = pop_data, .f = pluck)

# Seleccionar columnas relevantes
pop_data <- pop_data %>% 
    select(
        Geographic.Area.Name, 
        Estimate..SEX.AND.AGE..Total.population, 
        Year
    )
```

En esta sección, se identifican los archivos CSV que contienen "Data" en su nombre. Estos archivos se leen y manipulan para agregar una columna 'Year' a cada conjunto de datos. Luego, se combinan en un único marco de datos y se seleccionan las columnas relevantes.

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Obtener la lista de archivos .csv y .txt en el directorio
files_to_remove <- list.files(path = my_dir, pattern = "\\.(csv|txt)$", full.names = TRUE)

# Verificar si hay archivos para eliminar
if (length(files_to_remove) > 0) {
    # Eliminar los archivos
    file.remove(files_to_remove)
    cat("Archivos .csv y .txt eliminados correctamente en el directorio:", my_dir)
} else {
    cat("No hay archivos .csv y .txt para eliminar en el directorio:", my_dir)
}

pop_data 

```

Finalmente, se identifican los archivos temporales (CSV y TXT) y se eliminan. Se proporciona un mensaje indicando si se eliminaron archivos o si no se encontraron archivos para eliminar.

## Importación de Datos con Rapid API

Este código en R se desarrolló con el propósito de importar datos mediante una interfaz de programación de aplicaciones (API) para obtener información relacionada con códigos postales en los Estados Unidos.

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Configurar la URL y claves de la API
url <- "https://us-zip-code-to-income.p.rapidapi.com/"
API_Key <- "f7eec58e6cmsh805b1f341c11ff4p18a6b6jsn441ba456adeb" #uwu xd#
API_Host <- "us-zip-code-to-income.p.rapidapi.com"
request_delay <- 2 # Definir el retardo entre las solicitudes (en segundos)
```

En la sección inicial del código, se establecen las variables esenciales para la configuración de la API. Esto incluye la URL de la API, la clave de autenticación **(API_Key)**, el host de la API **(API_Host)**, y un retardo entre las solicitudes **(request_delay)** para cumplir con los límites de la API y evitar posibles restricciones.

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# missing_zip_codescrime_zip que se quiere extraer de la API
missing_zip_codescrime_zip <- c("48430", "48211", "48760", "48139", "48440")
```

```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
# Cargar la biblioteca
library(httr)
library(jsonlite)
library(tidyverse)
library(fs)
library(rvest)
library(xml2)
library(readxl)
library(caret)
library(dplyr)
```

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Definir una función para importar datos utilizando la API
data_import_IP <- function(url, API_Key, API_Host, request_delay) {
    
    data_df <- tibble()
    
    for (i in seq_along(missing_zip_codescrime_zip)) {
        
        query_params <- list(zip = missing_zip_codescrime_zip[i])
        
        # Realizar la solicitud GET a la API
        response <- GET(url, add_headers("X-RapidAPI-Key" =  API_Key, "X-RapidAPI-Host" = API_Host), query = query_params)
        
        # Verificar el estado de la respuesta
        stop_for_status(response)
        
        # Convertir la respuesta JSON a un marco de datos
        cont_response <- fromJSON(rawToChar(response$content))
        
        # Concatenar el marco de datos resultante
        data_df <- bind_rows(data_df, as_tibble(cont_response))
        
        # Imprimir información sobre el código postal procesado
        print(str_glue("Zip Code: {missing_zip_codescrime_zip[i]}"))
        
        # Retardo entre las solicitudes para evitar límites de la API
        Sys.sleep(request_delay)
    }
    
    # Nombrar las columnas del marco de datos resultante
    colnames(data_df) <- c("success", "zip", "House Hold Median Income", "household MeanIncome", "familyMedianIncome", "familyMeanIncome", "numHouseholds", "nonFamilyHousehold MedianIncome", "nonFamilyHouseholdMeanIncome", "familyPercentPoverty")
    
    return(data_df)
}
```

La función **'data_import_IP'** tiene como objetivo principal realizar solicitudes a una API para obtener información relacionada con códigos postales en los Estados Unidos. Aquí está una explicación paso a paso de lo que hace la función:

**1.Inicialización del Marco de Datos:**

-   Se crea un marco de datos vacío llamado **'data_df'** utilizando la función **'tibble()'** que se utilizará para almacenar los resultados de las solicitudes a la API.

**2.Bucle de Importación de Datos:**

-   La función utiliza un bucle **'for'** para iterar sobre cada código postal en la secuencia **'missing_zip_codescrime_zip'**. Este bucle permite procesar cada código postal de manera individual.

**3.Construcción de Parámetros de Consulta:**

-   Para cada código postal, se construye un conjunto de parámetros de consulta denominado **'query_params'**. Este conjunto incluye el código postal actual y se utiliza en la solicitud a la API.

**4.Solicitud GET a la API:**

-   Se realiza una solicitud GET a la API utilizando la función **'GET'**. La URL de la API, las cabeceras de autenticación (**'X-RapidAPI-Key'** y **'X-RapidAPI-Host'**), y los parámetros de consulta se utilizan en la solicitud. La respuesta de la API se almacena en la variable **'response'**.

**5.Procesamiento de la Respuesta JSON:**

-   La respuesta en formato JSON se convierte a un formato tabular utilizando la función **'fromJSON'** del paquete **'jsonlite'**. El resultado se almacena en el objeto **'cont_response'**.

**6.Concatenación de Resultados:**

-   Los resultados de cada solicitud se concatenan al marco de datos existente (**'data_df'**) utilizando la función **'bind_rows'**.

**7.Impresión de Información del Código Postal:**

-   Se imprime información descriptiva sobre el código postal procesado, utilizando la función **'print(str_glue("Zip Code: {missing_zip_codescrime_zip[i]}"))'**. Esto facilita el seguimiento del progreso y la identificación de los códigos postales procesados.

**8.Retardo entre Solicitudes:**

Para cumplir con las políticas de velocidad de la API y evitar exceder límites, se introduce un retardo de tiempo (**'Sys.sleep(request_delay)'**) entre cada solicitud.

**9.Nombrar Columnas del Marco de Datos Resultante:**

-   Se asignan nombres descriptivos a las columnas del marco de datos resultante (**'data_df'**) para una interpretación más clara de los datos.

**10.Retorno del Resultado:**

-   La función retorna el marco de datos final (data_df), que contiene la información recopilada de todas las solicitudes a la API.

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Llamar a la función de importación de datos utilizando la API
result_df <- data_import_IP(url, API_Key, API_Host, request_delay)

result_df
```

## Organización Modular en R: Creación y Aplicación de Funciones Personalizadas

En R, puedes crear tus propias funciones y guardarlas en un script separado para luego poder utilizarlas en otros scripts. Aquí hay un ejemplo paso a paso:

**1.Crea tu función en un script:** \* Abre un nuevo script en R (puedes usar un editor de texto o un entorno de desarrollo como RStudio). Define tu función y guárdala en el script. Por ejemplo, crea un archivo llamado **misFunciones.R** con el siguiente contenido:

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# misFunciones.R

sumaDosNumeros <- function(a, b) {
  resultado <- a + b
  return(resultado)
}

```

**2.Guarda el script:** \* Guarda el script con el nombre que hayas elegido, en este caso, **'misFunciones.R'**.

**3.Usa la función en otro script:** \* En otro script o en la consola de R, puedes cargar el script que contiene tus funciones usando la función **'source'**. Aquí hay un ejemplo:

```         
# scriptPrincipal.R

# Cargar el script que contiene las funciones
> source("misFunciones.R")

# Ahora puedes utilizar la función en este script
> resultado_suma <- sumaDosNumeros(3, 5)
> print(resultado_suma)
```

## Almacenando de data

En estas líneas de código se utilizan para almacenar el objeto **'pop_data'** en un archivo RDS y luego cargarlo de nuevo en el entorno de trabajo de R en el objeto **'pop_data'**. Este proceso es útil para persistir datos entre sesiones de trabajo o compartir datos de manera eficiente, ya que el formato RDS conserva la estructura y los tipos de datos del objeto original.

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Guardar en archivo RDS
rds_file <- file.path(my_dir, "pop_data.rds")
saveRDS(pop_data, rds_file)

# Cargar el archivo RDS en un objeto llamado pop_data
pop_data <- readRDS(file.path(my_dir, "pop_data.rds"))
```

# Manipulación y Preparación de Datos

## Comparación de Data Frames

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Definir valores de prueba para DWD_MI_Zip y crime_zip
DWD_MI_Zip <- c("10001", "20002", "30303", "40004", "50005")
crime_zip <- c("10001", "30303", "50005", "60006", "70007")

# Comprobar si los conjuntos de códigos postales son idénticos
identical_result <- identical(DWD_MI_Zip, crime_zip)
print(paste("Los conjuntos son idénticos:", identical_result))
```

La función **identical()** se utiliza para verificar si dos objetos son idénticos entre sí. La función devuelve un valor lógico (TRUE o FALSE) según si los objetos son idénticos o no.

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Mostrar los códigos postales que faltan en cada conjunto
missing_DWD_MI_Zip <- DWD_MI_Zip[!DWD_MI_Zip %in% crime_zip]
missing_crime_zip <- crime_zip[!crime_zip %in% DWD_MI_Zip]

print("Códigos postales que faltan en DWD_MI_Zip:")
print(missing_DWD_MI_Zip)

print("Códigos postales que faltan en crime_zip:")
print(missing_crime_zip)
```

Este código en R tiene como objetivo identificar y mostrar los códigos postales que faltan en dos conjuntos de datos, **DWD_MI_Zip** y **crime_zip**. Utiliza la función **%in%** para realizar la comparación entre los dos conjuntos y determinar los elementos que no están presentes en ambos.

## Transformar y formatear datos

La librería scales en R proporciona funciones para transformar y formatear datos, especialmente útiles para la visualización de gráficos. Aquí hay un resumen de las principales funciones y utilidades proporcionadas por scales:

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Cargar la biblioteca
library(scales)

# Numero de prueba
number <- 5.1234567

# Formato de dolares
number |> 
    scales::dollar()

```

```{r, message=FALSE , warning=FALSE, rows.print = 5}

# Formateo de Números como Porcentajes
number |> 
    scales::percent(accuracy = 0.01)

```

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Crear un vector de fechas
dates <- as.Date(c("2022-01-15 12:30:00", "2022-02-20 15:45:00", "2022-03-25 18:15:00"))

# Formatear las fechas en el estilo "día de la semana, mes día, año"
formatted_dates <- date_format("%A, %B %d, %Y")(dates)
print(formatted_dates)

```

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Crear un vector de horas
times <- as.POSIXct(c("2022-01-15 12:30:00", "2022-02-20 15:45:00", "2022-03-25 18:15:00"))

# Formatear las horas en el estilo "hora:minuto AM/PM"
formatted_times <- time_format("%I:%M %p")(times)
print(formatted_times)
```

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Crear un vector de fechas y horas
datetime <- as.POSIXct(c("2022-01-15 12:30:00", "2022-02-20 15:45:00", "2022-03-25 18:15:00"))

# Formatear las fechas y horas en el estilo "día de la semana, mes día, año hora:minuto AM/PM"
formatted_datetime <- date_format("%A, %B %d, %Y %I:%M %p")(datetime)
print(formatted_datetime)

```

## Filtrar datos

En R, la función **'filter'** se utiliza comúnmente para extraer filas específicas de un conjunto de datos basándose en condiciones específicas. Aquí tienes algunos ejemplos de cómo usar la función **'filter'**:

```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
library(dplyr)
```
```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Crear un ejemplo de dataframe
df <- data.frame(
  ID = c(1, 2, 3, 4, 5, 6),
  Name = c("Alice", "Bob", "Charlie", "David", "Eva", "Fernanda"),
  Age = c(25, 30, 22, 35, 28, NaN),
  Gender = c("Female", "Male", "Male", "Male", "Female", "Female")
)

df
```

-   Filtrar basado en una condición numérica

```{r}
# Filtrar personas mayores de 25 años
result <- filter(df, Age > 25)

result
```

-   Filtrar basado en una condición de texto

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Filtrar personas de género femenino
result <- filter(df, Gender == "Female")

result
```

-   Filtrar utilizando múltiples condiciones

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Filtrar personas mayores de 25 años y de género femenino
result <- filter(df, Age > 25 & Gender == "Female")

result

```

-   Filtrar valores faltantes

```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 6}
library(naniar)
```

```{r, message=FALSE , warning=FALSE, rows.print = 6}
# Resumen de valores faltantes en el conjunto de datos filtrado
miss_val_resumen <- df |>
  miss_case_summary()

# Imprimir el resumen
miss_val_resumen
```

## Uso de mutate

La función **mutate()** es parte del paquete **dplyr** en R y se utiliza para realizar transformaciones en columnas existentes o agregar nuevas columnas a un conjunto de datos. Esta función es comúnmente utilizada en el contexto de manipulación y transformación de datos.

```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
# Instala e carga las librerías necesarias si no las has instalado
library(dplyr)
library(lubridate)
```

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Crear un conjunto de datos ficticio con una columna de fechas
datos <- data.frame(
  Date_chr = c("2022-01-15", "2021-08-22", "2023-03-10")
)

# Utilizar mutate() y year() para crear una nueva columna "Year"
datos_con_year <- datos %>%
  mutate(Year = year(ymd(Date_chr)))

# Imprimir el conjunto de datos resultante
datos_con_year
```

## Técnicas de Fusión y Transformaciones

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Crear ejemplo de df1
df1 <- data.frame(
  ID = c(1, 2, 3, 4, 2, 3, 4),
  Nombre = c("Alice", "Bob", "Charlie", "David","Bob", "Charlie", "David"),
  Apellido = c("Smith", "Johnson", "Garcia", "Fisher","Johnson", "Garcia", "Fisher")
)

# Crear ejemplo de df2
df2 <- data.frame(
  ID = c(2, 3, 4, 5, 3, 4, 2),
  Ventas = c(2250, 5047, 5461, 7539, 2250, 5047, 5461)
)

```

-   Fusión de DF

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Ejemplo de fusión 

merged_df <- df1 |>
  full_join(df2, by = "ID")

merged_df
```

-   Agrupacion

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Ejemplo de agrupación y resumen 

ventas_resumen <- merged_df %>%
  group_by(Nombre) %>%
  summarise(Total_Ventas = sum(Ventas))

ventas_resumen
```

-   Combinación de columnas

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Ejemplo de combinación de columnas
merged_df <- merged_df %>%
  mutate(Nueva_Columna = paste(Nombre, Apellido, sep = "-"))

merged_df
```

## Selección de Información Relevante

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Seleccionar solo las columnas "Nombre" y "Ventas"
datos_seleccionados <- select(merged_df, Nombre, Ventas)

# Imprimir el dataframe resultante
datos_seleccionados
```

# Visualizaciones de Datos

Los gráficos desempeñan un papel fundamental en el análisis de datos y la visualización de resultados en el ámbito de la programación estadística, y R se destaca como un lenguaje de programación especialmente poderoso para la creación de gráficos.

```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
# Cargar bibliotecas necesarias
library(readxl)
library(tidyverse)
library(naniar)
library(lubridate)
library(stringr)
library(fs)
library(scales)
library(ggplot2)
library(plotly)
library(tidyquant)
library(ggrepel)
library(purrr)

# Fuente de las funciones personalizadas
source("06_funciones_y_interacion_iteration/Zillow_PC_Function.R")
source("06_funciones_y_interacion_iteration/funciones_de_importacion.R")
```

```{r, message=FALSE , warning=FALSE, rows.print = 5}

# Obtener datos de ciudades con la Tasa de Crecimiento Anual Compuesta (CAGR) para los años 2000 a 2022
CAGR_Data <- getTopCitiesByCAGR(2000, 2022, 30)

# Importar datos del Índice del Mercado Inmobiliario de Zillow (ZMI)
ZMI_Data_Prep <- data_import("ZMI_data_pre")

# Combinar los marcos de datos CAGR_Data y ZMI_Data_Prep en la columna "City_zip"
ZillowTrends <- CAGR_Data |> left_join(ZMI_Data_Prep, by = "City_zip")

ZillowTrends
```

## Exploración de datos perdidos

Utilizando la función 'gg_miss_var' de la biblioteca 'naniar' para visualizar los valores perdidos en los datos.

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Grafico de valores faltantes
ZillowTrends |> gg_miss_var()
```

## Apreciación de Precios de Viviendas Año tras Año

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Seleccionar las columnas relevantes en el marco de datos ZillowTrends
ZillowTrends <- ZillowTrends |>
    select(City_zip, Year, Z_Home_Value_Index, ZHVI_PC_YoY, ZHVI_PC_YoY_chr, timeperiod, Number_of_Years, CAGR)

# Calcular la tasa de crecimiento promedio (CAGR) para todos los datos
ave_rate <- ZillowTrends |>
    pull(CAGR) |>
    mean() |>
    scales::percent(accuracy = 0.01)

# Obtener el año mínimo y máximo en el conjunto de datos ZillowTrends
min_y <- ZillowTrends |>
    pull(Year) |>
    min()

max_y <- ZillowTrends |>
    pull(Year) |>
    max()

# Agregar una nueva columna 'tool_tip' con información para etiquetas emergentes
ZillowTrends <- ZillowTrends |>
    mutate(tool_tip = str_glue("CityZip: {City_zip},
                               %Change: {ZHVI_PC_YoY_chr},
                               Year: {Year}"))

# Crear un gráfico ggplot
g <- ZillowTrends |> ggplot(aes(x = Year, y = ZHVI_PC_YoY, color = City_zip, text = tool_tip, group = City_zip)) +
    geom_line(linewidth = 1) +
    geom_point(size = 2, alpha = 0.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "black", linewidth = 0.50) +
    labs(
        title = "Apreciación de Precios de Viviendas Año tras Año",
        subtitle = str_glue("Cambio porcentual YoY de {min_y} a {max_y}"),
        x = "AÑO",
        y = "Cambio Porcentual (%)",
        caption = str_glue("De {min_y} a {max_y}, la CAGR promedio ha sido {ave_rate} \n para estos 30 principales códigos postales en el Área Metropolitana de Detroit")
    )

# Mostrar el gráfico
g

```

##Convierte el gráfico a un gráfico interactivo

Convierte el gráfico creado con ggplot2 a un gráfico interactivo utilizando plotly y especifica que las etiquetas de herramientas deben tomarse de la columna 'tool_tip'.

```{r, message=FALSE , warning=FALSE, rows.print = 5}
## Gráfico interactivo
g |> 
    ggplotly(tooltip = "tool_tip") 
```

## Apreciación de Precios de Viviendas Año tras Año 2

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Crear un gráfico ggplot con facetas (Plot 2)
g <- ZillowTrends |> ggplot(aes(x = Year, y = ZHVI_PC_YoY, color = City_zip, text = tool_tip, group = City_zip)) +
    geom_line(linewidth = 1) +  # Agregar líneas para representar los datos
    geom_point(size = 2, alpha = 0.5) +  # Agregar puntos para representar los datos
    geom_hline(yintercept = 0, linetype = "dashed", color = "black", linewidth = 0.50) +  # Agregar línea horizontal en y=0
    facet_wrap(~ City_zip, scales = "free_y") +  # Facetar el gráfico por código postal con escalas independientes en el eje y
    labs(
        title = "Apreciación de Precios de Viviendas Año tras Año",
        subtitle = str_glue("Cambio porcentual YoY de {min_y} a {max_y}"),
        x = "AÑO",
        y = "Cambio Porcentual (%)",
        caption = str_glue("De {min_y} a {max_y}, la CAGR promedio ha sido {ave_rate} \n para estos 30 principales códigos postales en el Área Metropolitana de Detroit")
    ) +
    theme_minimal() +  # Puedes ajustar el tema según tus preferencias
    theme(
        plot.title = element_text(size = 20, face = "bold"),
        axis.title.x = element_text(size = 16),
        axis.title.y = element_text(size = 16),
        axis.text.x = element_text(size = 6),
        axis.text.y = element_text(size = 6),
        legend.text = element_text(size = 12)
    ) +
    guides(color = guide_legend(override.aes = list(size = 4)))  # Ajustar el tamaño de la leyenda

g
```

```{r, message=FALSE , warning=FALSE, rows.print = 5}

# Convertir el gráfico ggplot a una versión interactiva usando plotly
g_plotly <- g |> ggplotly(tooltip = "tool_tip")

# Ajustar el tamaño del gráfico plotly
g_plotly <- g_plotly %>%
    config(displayModeBar = TRUE) %>%  # Puedes desactivar la barra de herramientas si lo prefieres
    layout(width = 1000, height = 600)  # Ajustar el tamaño según tus preferencias

# Mostrar el gráfico plotly
g_plotly

```

## Precios de Viviendas vs Empleo
```{r, message=FALSE , warning=FALSE, rows.print = 5}

# Importar datos relevantes para el Plot 3
ZMI_Data_Prep
Employment_C <- data_import("Employment_C")

# Calcular el promedio del cambio porcentual YoY del índice de valor de la vivienda (ZHVI_PC_YoY) por año
avg_zhvi_pc_by_year <- ZMI_Data_Prep |>
    group_by(Year) |> 
    summarise("Home Value Index" = ZHVI_PC_YoY |> mean())

# Combinar datos de empleo y datos del índice de valor de la vivienda por año
homev_vs_Job_growth <- Employment_C |>
    left_join(avg_zhvi_pc_by_year, by = "Year") |>
    select(Year, PC_YoY, "Home Value Index") |>
    mutate("Employment in Metro Detroit" = PC_YoY) |>
    gather(key = "Legend", value = "Percentage_Change", "Employment in Metro Detroit", "Home Value Index")

# Modificar el formato de porcentaje y la etiqueta del texto
homev_vs_Job_growth <- homev_vs_Job_growth |>
    mutate(PC_YoY_chr = Percentage_Change |> scales::percent(accuracy = 0.01)) |>
    mutate(Percentage_Change = (Percentage_Change * 100)) |>
    mutate(label_text = str_glue("Legend:{Legend},
                                 Year:{Year},
                                 Percentage:{PC_YoY_chr}"))

# Calcular el número de códigos postales únicos en ZMI_Data_Prep
num_of_zip <- ZMI_Data_Prep |> select(City_zip) |> unique() |> nrow()

# Crear el gráfico ggplot para el Plot 3
p3 <- homev_vs_Job_growth |>
    drop_na(Percentage_Change) |>
    ggplot(aes(x = Year, y = Percentage_Change, linetype = Legend, colour = Legend, label = label_text)) +
    geom_line(linewidth = 1) +
    geom_point(aes(text = label_text), size = 2) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray0", size = 0.50)+
    labs(
        title = "Precios de Viviendas vs Empleo",
        subtitle = "Cambio Porcentual YoY entre Precios de Viviendas y Empleo",
        y = "Cambio Porcentual (%)",
        caption = str_glue("Las fuentes de datos son 'Zillow' y 'Bureau of Labor Statistics'.
        Los datos de empleo son del área de Empleo de Detroit-Warren-Dearborn. 
        El Índice de Valor de la Vivienda es la media del cambio de {num_of_zip} códigos postales en el área de Detroit-Warren-Dearborn.'"
    )) +
    theme_light()

# Convertir el gráfico ggplot a una versión interactiva usando plotly
p3_plotly <- p3 |> ggplotly(tooltip = "text")

# Mostrar el gráfico plotly
p3_plotly

```

```{r, message=FALSE , warning=FALSE, rows.print = 5}

p3 + scale_colour_brewer(palette = "Set1")

```

```{r, message=FALSE , warning=FALSE, rows.print = 5}

p3 + theme (
    
    plot.title    = element_text(color="dark blue", size = 12, face = "bold"),
    plot.subtitle = element_text(color="dark blue", size = 10, face = "italic"),
    plot.caption  = element_text(color="dark blue", size = 8, face = "bold.italic")
)    
    
```


## Regresión LOESS
```{r, message=FALSE , warning=FALSE, rows.print = 5}

## (LOESS) Local Polynomial Regression

# Combinar datos de empleo y datos del índice de valor de la vivienda por año
homev_vs_Job_growth_II <- Employment_C |>
    left_join(avg_zhvi_pc_by_year, by = "Year") |>
    select(Year, PC_YoY, "Home Value Index") |>
    mutate(Employment = PC_YoY) |>
    mutate(Home_Value_Index = `Home Value Index`) |>
    filter(Year > 1999)

# Ajustar un modelo LOESS (Local Polynomial Regression)
mod <- loess(Home_Value_Index ~ Employment, data = homev_vs_Job_growth_II)
grid <- tibble(Employment = seq(min(homev_vs_Job_growth_II$Employment), max(homev_vs_Job_growth_II$Employment), length = 50))
grid$Home_Value_Index <- predict(mod, newdata = grid)

# Calcular residuos estandarizados y identificar valores atípicos
std_resid <- resid(mod) / mod$s
outlier <- filter(homev_vs_Job_growth_II, abs(std_resid) > 1)
outlier

```

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Crear un gráfico ggplot para el modelo LOESS

p4 <- ggplot(homev_vs_Job_growth_II, aes(Employment, Home_Value_Index)) +
    geom_point() +
    geom_line(data = grid, colour = "blue", size = 1.5) + 
    ggrepel::geom_text_repel(data = outlier, aes(label = Year)) +
    
    labs(
        title = "Regresión LOESS",
        y = "Cambio Promedio % Índice de Valor de la Vivienda",
        x = "Cambio Promedio % Empleo",
        caption = "La regresión LOESS, a veces llamada regresión local, es un método que utiliza el ajuste local para ajustar un modelo de regresión a un conjunto de datos"
    )

# Añadir anotaciones al gráfico
p4 + annotate(geom = "curve",
              x = -0.10,
              y = 0.09,
              xend = -0.106,
              yend = 0.055,
              curvature = 0.3, arrow = arrow(length = unit(2, "mm"))) +
    annotate(geom = "text", x = -0.10, y = 0.09, label = "Pandemia Global", hjust = "left")

```


```{r, message=FALSE , warning=FALSE, rows.print = 5}

## LOESS & ggplotly

# Modificar el formato de porcentaje y la etiqueta del texto para ggplotly
homev_vs_Job_growth_II <- homev_vs_Job_growth_II |>
    mutate(Home_Value_Index_ch = Home_Value_Index |> scales::percent(accuracy = 0.01)) |>
    mutate(Employment_ch = PC_YoY |> scales::percent(accuracy = 0.01)) |>
    mutate(label_text  = str_glue("Year: {Year},
                                   Home Value Index: {Home_Value_Index_ch},
                                   Employment: {Employment_ch}"))

# Crear un gráfico ggplotly interactivo
p4 <- ggplot(homev_vs_Job_growth_II, aes(Employment, Home_Value_Index)) +
    geom_point(aes(text = label_text), size = 2) +
    geom_line(data = grid, colour = "blue", size = 1.5) + 
    ggrepel::geom_text_repel(data = outlier, aes(label = Year)) +
    
    labs(
        title = "Regresión LOESS",
        y = "Cambio Promedio % Índice de Valor de la Vivienda",
        x = "Cambio Promedio % Empleo",
        caption = "La regresión LOESS, a veces llamada regresión local, es un método que utiliza el ajuste local para ajustar un modelo de regresión a un conjunto de datos"
    ) +
    scale_y_continuous(labels = scales::percent) +
    scale_x_continuous(labels = scales::percent)

# Convertir el gráfico ggplotly a una versión interactiva
p4 |>
    ggplotly(tooltip = "text")

```


## Explorando Mercados Inmobiliarios: Usando Geom_Bar

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Crear un conjunto de datos combinando datos de empleo y datos del índice de valor de la vivienda por año (Plot 5)
homev_vs_Job_growth_III <- Employment_C |>
    left_join(avg_zhvi_pc_by_year, by = "Year") |>
    select(Year, PC_YoY, "Home Value Index") |>
    mutate(Employment = PC_YoY) |>
    mutate(Home_Value_Index = `Home Value Index`)

# Añadir nuevas columnas con porcentajes y texto para etiquetas
homev_vs_Job_growth_III <- homev_vs_Job_growth_III |>
    mutate(Home_Value_Index_PC = `Home Value Index` |> scales::percent(accuracy = 0.01)) |>
    mutate(Employment_PC = PC_YoY |> scales::percent(accuracy = 0.01)) |>
    mutate(label_text1 = str_glue("Employment: {Employment_PC}")) |>
    mutate(label_text2 = str_glue("Home Value Index: {Home_Value_Index_PC}")) |>
    mutate(label_text3 = str_glue("E: {Employment_PC}%")) |>
    mutate(label_text4 = str_glue("H: {Home_Value_Index_PC}%"))

# Obtener el rango de años
miny <- homev_vs_Job_growth_III |> pull(Year) |> min()
max_y <- homev_vs_Job_growth_III |> pull(Year) |> max()

# Crear un gráfico de barras apiladas
p5 <- ggplot(homev_vs_Job_growth_III) +
    geom_bar(
        aes(Year, Employment, text = label_text1),
        stat = "identity",
        position = "dodge",
        fill = "cadetblue1",
        color = "black"
    ) +
    
    geom_bar(
        aes(Year, Home_Value_Index, text = label_text2),
        stat = "identity",
        position = "dodge",
        fill = "navy",
        color = "black",
        alpha = 0.3
    ) +
    
    scale_y_continuous(labels = scales::percent) +
    
    labs(
        title = "Precios de Viviendas vs Empleo",
        caption = str_glue("Fuentes de datos: Zillow y Bureau Labor of Statistics \n Marco temporal: {min_y} {max_y}")
    ) +
    
    geom_label(
        aes(Year, Employment, label = label_text3),
        hjust = "left",
        vjust = "top",
        size = 3
    ) +
    
    geom_label(
        aes(Year, Home_Value_Index, label = label_text4),
        hjust = "right",
        vjust = "bottom",
        size = 3
    )

# Convertir el gráfico ggplot a una versión interactiva usando plotly
p5 |> ggplotly(tooltip = "text") |> 
    layout(
        title = list(
            text = paste(
                'Precios de Viviendas vs Empleo',
                '<br>',
                '<sup>',
                '<b>', 'Fuentes de datos:',
                '</b>', "(Zillow) & (Bereau Labor of Statistics)",
                '</sup>'
            )
        )
    )

```


## Representación Gráfica del Ingreso Medio Familiar

```{r, message=FALSE , warning=FALSE, rows.print = 5}

# Extraer códigos postales y convertirlos a formato entero
CAGR_Data_zip <- CAGR_Data |>
    mutate(zip = str_extract(City_zip, pattern = "\\d{5}")) |>
    mutate(zip = zip |> as.integer())

# Importar datos demográficos
demographics <- data_import(dataSource = "Demographis")

# Combinar datos de CAGR con datos demográficos utilizando códigos postales
demographics_CAGR <-  CAGR_Data_zip |> left_join(demographics, by = "zip")

# Visualizar la estructura de datos
demographics_CAGR |> glimpse()
```


```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Agregar una columna de texto para etiquetas
demographics_CAGR <- demographics_CAGR |>
    mutate(label_text = str_glue("Household Median Income: {House_Hold_Median_Income |> scales::dollar()}"))

# Crear un gráfico de barras
p6 <- ggplot(demographics_CAGR, aes(x = reorder(City_zip, -House_Hold_Median_Income))) +
    geom_bar(
        aes(y = House_Hold_Median_Income, text = label_text),
        stat = "identity",
        color = "black", fill = "skyblue"
    ) +
    scale_y_continuous(name = "Median Income", labels = scales::dollar) +
    labs(x = "ZipCode") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 90))

# Convertir el gráfico ggplot a una versión interactiva usando plotly
g_plotly <- p6 |> ggplotly(tooltip = "label_text") |> 
    layout(
        title = list(
            text = paste0('<br>',
                          'Household Median Income',
                          '</br>',
                          '<br>',
                          '<sup>',
                          '<b>',
                          'Note:',
                          '</b>', 'These ZipCodes have the higher CAGR in Metro Detroit',
                          '</sup>')
        )
    )

# Ajustar el tamaño del gráfico plotly
g_plotly <- g_plotly %>%
    config(displayModeBar = TRUE) %>%  # Puedes desactivar la barra de herramientas si lo prefieres
    layout(width = 1000, height = 600)  # Ajustar el tamaño según tus preferencias

# Mostrar el gráfico plotly
g_plotly
```


## Gráfico de Pareto Clásico

El Gráfico de Pareto es una herramienta visual poderosa utilizada en gestión y mejora de procesos para identificar y priorizar problemas o causas, centrándose en aquellos que tienen el mayor impacto. Su nombre proviene del principio de Pareto, que sugiere que aproximadamente el 80% de los problemas provienen del 20% de las causas. 

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Data Preparation
data <- data.frame(
    Category = c("Dose Missed",
                 "Wrong Time",
                 "Overdose",
                 "Wrong Patient",
                 "Wrong Drug",
                 "Wrong Calculation",
                 "Duplicated Dose",
                 "Under Dose",
                 "Wrong Rate",
                 "Technique Error",
                 "Unauthorized Drug"),
    Frequency = c(92, 83, 76, 59, 53, 27, 16, 9, 7, 4, 3)
)

# Ordenar los datos por frecuencia en orden descendente
data <- data |>
    arrange(desc(Frequency)) |>
    # Calcular la suma acumulativa y el porcentaje acumulativo
    mutate(Cumulative = cumsum(Frequency)) |>
    mutate(Cumulative_Percent = Cumulative / sum(Frequency) * 100)

# Crear un gráfico de barras y línea (Pareto Chart)
p <- ggplot(data, aes(x = reorder(Category, -Frequency), y = Frequency)) +
    geom_bar(
        aes(text = paste("Category:", Category, "<br>Frequency:", Frequency)),
        stat = "identity",
        fill = "blue"
    ) +
    geom_line(
        aes(y = Cumulative_Percent,
            group = 1,
            text = paste("Category:", Category, "<br>Cumulative %:", sprintf("%.2f", Cumulative_Percent))),
        colour = "red"
    ) +
    geom_point(
        aes(y = Cumulative_Percent,
            text = paste("Category:", Category, "<br>Cumulative %:", sprintf("%.2f", Cumulative_Percent))),
        colour = "red"
    ) +
    # Agregar una línea de referencia en el 80% para el Pareto
    geom_hline(aes(yintercept = 80), linetype = "dashed", color = "green") +
    # Agregar eje secundario para el porcentaje acumulativo
    scale_y_continuous(sec.axis = sec_axis(~ ., name = "Cumulative %")) +
    theme_minimal() +
    labs(
        x = "Type of Medication Errors",
        y = "Frequency #",
        title = str_glue("Pareto Chart Types of Medication Errors (n = {data$Frequency |> sum()})")
    ) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Convertir el gráfico ggplot a una versión interactiva usando plotly
p |>
    ggplotly(tooltip = "text")

```

# Modelos con LOWESS

LOWESS, que significa "locally weighted scatterplot smoothing" (alisado de gráficos de dispersión ponderado localmente), es un método estadístico utilizado para realizar un suavizado local en datos bidimensionales. LOWESS es un tipo de modelo no paramétrico que se utiliza para explorar la relación entre dos variables, generalmente representadas en un gráfico de dispersión.

El método LOWESS suaviza los datos mediante la aplicación de pesos locales a los puntos de datos en función de su proximidad espacial. Esto significa que el suavizado en cada punto se realiza teniendo en cuenta principalmente los puntos cercanos y asignando pesos más bajos a los puntos más alejados. De esta manera, el modelo se adapta localmente a la variabilidad de los datos y proporciona un suavizado más flexible que algunos métodos globales de suavizado.

Para aplicar el modelo LOWESS, se selecciona un parámetro llamado "ancho de banda" que determina la ventana alrededor de cada punto que se utilizará para calcular el suavizado local. Un ancho de banda más grande incluirá más puntos en el cálculo, lo que dará como resultado un suavizado más suave pero menos sensible a las variaciones locales.

LOWESS es útil en la exploración visual de datos para identificar tendencias o patrones subyacentes, especialmente cuando los datos muestran variabilidad significativa. Este método ha sido utilizado en diversas disciplinas, como estadísticas, econometría y ciencias sociales, para comprender mejor la relación entre variables y detectar tendencias locales en datos ruidosos.

```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
# Carga de bibliotecas ----
library(tidyverse)  # Paquete para manipulación y visualización de datos
library(lubridate)  # Paquete para manipulación de fechas
library(stringr)    # Paquete para manipulación de cadenas de texto
library(fs)         # Paquete para operaciones con el sistema de archivos
library(scales)     # Paquete para escalas personalizadas en gráficos

# Carga de la biblioteca ggplot2 si no está cargada
library(ggplot2)
```

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Establecer una semilla para reproducibilidad
set.seed(13)

# Generar datos de ejemplo ----
x <- seq(1, 200, length.out = 200)  # Secuencia de valores para x
y <- sin(x/10) + rnorm(200, 0, 0.5)  # Función seno con ruido aleatorio

# Aplicar suavizado LOESS ----
smoothed_data <- lowess(x, y)

# Graficar los datos originales y suavizados ----
P1 <- ggplot(mapping = aes(x = x, y = y)) +
    geom_point(color = "blue") +
    geom_line(aes(y = smoothed_data$y), color = "red") +
    ggtitle("LOESS")

P1
```

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Generar tress modelos con diferentes spans
# Entre mas bajo el span mas alta la posibilidad de overfitting
# Cross validation para encontrar el mejor span

model1 <- loess(y ~ x, span = 0.2)
model2 <- loess(y ~ x, span = 0.5)
model3 <- loess(y ~ x, span = 0.8)

# Grafico de comparacion de modelos ----

FP <- ggplot(mapping = aes(x = x, y = y)) +
    geom_point(color = "blue") +
    geom_line(aes (y = fitted (model1)), color = "red")+
    geom_line(aes (y = fitted (model2)), color = "black")+
    geom_line(aes (y = fitted (model3)), color = "yellow")+
    ggtitle("LOESS")

library(plotly)
FP |> ggplotly()


```


# Manejar Valores atípicos

La importancia de gestionar valores atípicos radica en varios aspectos clave. En primer lugar, estos valores pueden distorsionar las medidas estadísticas comunes, como la media y la desviación estándar, generando estimaciones sesgadas y poco representativas de la realidad. Al abordar los valores atípicos, se obtiene una visión más precisa y equilibrada de la distribución de los datos.

Además, los valores atípicos pueden influir significativamente en los modelos predictivos y algoritmos de aprendizaje automático. Si no se gestionan adecuadamente, pueden llevar a la creación de modelos ineficaces o poco precisos. Identificar y tratar estos valores atípicos permite mejorar la robustez y la generalización de los modelos, garantizando un rendimiento más confiable en datos nuevos.


```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
# Data completa

model_data <- read.csv("C:\\Users\\Usuario\\Documents\\R_class\\Analysis_&_Manipulation_De_Datos_En_Espanol\\01_Informacion_Data\\model_data.csv")
library(DT)
datatable(model_data, options = list(pageLength = 10), filter = "top")

```
```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Data completa
dim(model_data)

```
## Identificar valores atípicos utilizando la desviación estándar (SD)

Eliminar valores atípicos utilizando la desviación estándar (SD) es un enfoque común en estadística para identificar y tratar puntos de datos que se desvían significativamente de la media de un conjunto de datos. El método implica calcular la desviación estándar de los datos y eliminar los valores que están a una cierta distancia (múltiplos de la desviación estándar) de la media.

Aquí hay una descripción básica de cómo funciona este método:

Calcular la Media y la Desviación Estándar:

Se calcula la media (promedio) y la desviación estándar de los datos.
Establecer un Umbral:

Se elige un múltiplo de la desviación estándar como umbral. Comúnmente, se utilizan múltiplos como 1, 1.5, 2, o 3, dependiendo de la rigurosidad que se desee aplicar.
Identificar y Eliminar Valores Atípicos:

Se identifican los valores que están más allá del umbral establecido en ambos extremos de la distribución (por encima y por debajo de la media).
Estos valores se consideran atípicos y se eliminan del conjunto de datos.
Revisar y Validar:

Después de eliminar los valores atípicos, es importante revisar el conjunto de datos resultante y considerar si la exclusión de esos valores es justificada. En algunos casos, los valores atípicos pueden contener información importante o representar eventos genuinos.
Es importante destacar que este método asume que los datos siguen una distribución normal. 

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Calcular la media y la desviación estándar de la columna 'price' en el DataFrame 'model_data'
mean_price <- mean(model_data$price, na.rm = T)  # Calcular la media, ignorando los valores NA
sd_price <- sd(model_data$price, na.rm = T)  # Calcular la desviación estándar, ignorando los valores NA

# Identificar los valores atípicos basados en la desviación estándar (3 veces la desviación estándar)
outliers <- model_data$price < (mean_price - 3 * sd_price) | model_data$price > (mean_price + 3 * sd_price)

# Crear un nuevo DataFrame 'model1_no_outliers_sd_method' sin los valores atípicos
model1_no_outliers_sd_method <- model_data[!outliers, ]

# Mostrar información sobre el nuevo DataFrame utilizando la función 'glimpse()'
model1_no_outliers_sd_method |> glimpse()

```
## Eliminar valores atipicos con el metodo IQR

Eliminar valores atípicos con el método IQR (rango intercuartílico) es otra técnica común en estadística. El rango intercuartílico es la diferencia entre el tercer cuartil (Q3) y el primer cuartil (Q1) de un conjunto de datos. Los valores atípicos se identifican y eliminan si están por debajo de Q1 - 1.5 * IQR o por encima de Q3 + 1.5 * IQR. Aquí hay un ejemplo de cómo implementar esto en R:

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Calcular el primer cuartil (Q1)
Q1 <- quantile(model_data$price, 0.25)

# Calcular el tercer cuartil (Q3)
Q3 <- quantile(model_data$price, 0.75)

# Calcular el rango intercuartílico (IQR)
IQR <- Q3 - Q1

# Identificar los valores atípicos utilizando el método IQR
outliers <- model_data$price < (Q1 - 1.5 * IQR) | model_data$price > (Q3 + 1.5 * IQR)

# Crear un nuevo DataFrame sin valores atípicos
model2_no_outliers_IQR_method <- model_data[!outliers, ]

# Mostrar información sobre el nuevo DataFrame sin valores atípicos
glimpse(model2_no_outliers_IQR_method)

```


## Usar método Robustos

El Método de Desviación Absoluta de la Mediana (Median Absolute Deviation, MAD) es un enfoque robusto utilizado para identificar valores atípicos en un conjunto de datos. A diferencia de otros métodos que dependen de la media y la desviación estándar, el MAD utiliza la mediana como medida central y evalúa la dispersión de los datos en términos de desviaciones absolutas respecto a la mediana.

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Calcular la Desviación Absoluta de la Mediana (MAD)
MAD <- mad(model_data$price, constant = 1)

# Identificar valores atípicos utilizando el método robusto MAD
outliers <- abs(model_data$price - median(model_data$price)) / MAD > 2

# Crear un nuevo DataFrame sin valores atípicos utilizando el método robusto
model3_no_outliers_robust_method <- model_data[!outliers, ]

# Mostrar información sobre el nuevo DataFrame sin valores atípicos
model3_no_outliers_robust_method |> glimpse()

```

# Comprobación de linealidad 

La comprobación de linealidad se refiere a la evaluación de la relación entre variables en un modelo estadístico o matemático para determinar si esta relación es lineal. En el contexto de análisis de regresión, la linealidad es una suposición fundamental. La comprobación de linealidad implica asegurarse de que la relación entre la variable dependiente y las variables independientes pueda ser bien aproximada mediante una línea recta.


```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
library(gridExtra)
```
```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Lista de variables a incluir en los gráficos de dispersión
variables <- list("bathrooms", "bedrooms", "squareFootage", "zipCode")

# Función para crear gráficos de dispersión
create_scatter_plot_fun <- function(variables) {
    ggplot(model1_no_outliers_sd_method, aes(!!sym(variables), price)) +
        geom_point() +  # Añadir puntos al gráfico
        labs(title = paste("Gráfico de dispersión de precio vs", variables),
             x = variables,
             y = "Precio")
}

# Crear una lista de gráficos utilizando la función map
plots <- map(variables, create_scatter_plot_fun)

# Organizar los gráficos en una cuadrícula
grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], ncol = 2)

```
#  Gráficos de Residuos

Los gráficos de residuos son herramientas visuales utilizadas en el análisis de regresión para evaluar la calidad del ajuste de un modelo a los datos observados. Los residuos son las diferencias entre los valores observados y los valores predichos por el modelo. Al examinar los patrones y distribuciones de estos residuos, se pueden identificar posibles violaciones de las suposiciones del modelo de regresión.

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Lista de conjuntos de datos que contienen modelos sin valores atípicos
data_sets <- list(
  "model1_no_outliers_sd_method" = model1_no_outliers_sd_method,
  "model2_no_outliers_IQR_method" = model2_no_outliers_IQR_method,
  "model3_no_outliers_robust_method" = model3_no_outliers_robust_method
)

# Fórmula del modelo de regresión lineal
formula <- price ~ bathrooms + bedrooms + squareFootage + zipCode

# Ajustar modelos de regresión lineal a cada conjunto de datos
models <- map(data_sets, ~ lm(formula, data = .x))

# Función para crear gráficos de residuos vs valores ajustados
create_residuals_plot_fun <- function(model, model_name) {
  ggplot() +
    geom_point(aes(x = model$fitted.values, y = model$residuals)) +
    labs(
      title = paste("Residuos vs Valores Ajustados para", model_name),
      x = "Valores Ajustados",
      y = "Residuos"
    )
}

# Crear gráficos de residuos vs valores ajustados para cada modelo
plots <- map2(models, names(models), create_residuals_plot_fun)

# Organizar los gráficos en una cuadrícula de 2 columnas
grid.arrange(plots[[1]], plots[[2]], plots[[3]], ncol = 2)

```
## Normal Q - Q

Un Q-Q plot compara la distribución de los residuos con una distribución teórica (generalmente la distribución normal). Si los puntos del Q-Q plot siguen aproximadamente una línea diagonal, sugiere que los residuos se distribuyen de manera normal.


```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
# Lista de modelos de regresión lineal ajustados a conjuntos de datos específicos
library(ggpubr)
library(cowplot)
```
```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Lista de modelos de regresión lineal ajustados a conjuntos de datos específicos
models <- list(
  "model1_no_outliers_sd_method" = lm(price ~ bathrooms + bedrooms + squareFootage + zipCode, data = model1_no_outliers_sd_method),
  "model2_no_outliers_IQR_method" = lm(price ~ bathrooms + bedrooms + squareFootage + zipCode, data = model2_no_outliers_IQR_method),
  "model3_no_outliers_robust_method" = lm(price ~ bathrooms + bedrooms + squareFootage + zipCode, data = model3_no_outliers_robust_method)
)

# Función para crear gráficos Q-Q plot para los residuos de un modelo
create_qq_plot <- function(model_name, model) {
  residuals <- model$residuals
  qqplot <- ggqqplot(residuals) +
    ggtitle(paste("Gráfico Q-Q para", model_name))
  return(qqplot)
}

# Crear gráficos Q-Q plot para cada modelo
plots <- map2(names(models), models, create_qq_plot)

# Organizar los gráficos en una cuadrícula de 2 columnas usando plot_grid
combined_plot <- plot_grid(plotlist = plots, ncol = 2)
print(combined_plot)

```

# Coeficiente de correlación 

El coeficiente de correlación es una medida estadística que cuantifica la relación lineal entre dos variables. En otras palabras, el coeficiente de correlación indica en qué medida las variaciones en una variable están asociadas con las variaciones en otra variable. Esta medida se utiliza comúnmente para evaluar la fuerza y dirección de la relación entre dos variables continuas.

Existen diferentes tipos de coeficientes de correlación, pero uno de los más comunes es el coeficiente de correlación de Pearson. El coeficiente de correlación de Pearson, denotado como "r", tiene un rango de -1 a 1, donde:

* 1: Indica una correlación perfecta positiva. Esto significa que a medida que una variable aumenta, la otra variable también aumenta en una relación lineal perfecta.

* 0: Indica ausencia de correlación lineal. No hay relación lineal entre las dos variables.

* -1: Indica una correlación perfecta negativa. Esto significa que a medida que una variable aumenta, la otra variable disminuye en una relación lineal perfecta.

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Calcular el coeficiente de correlación entre price y bathrooms
cor_bathrooms <- cor(model1_no_outliers_sd_method$price, model1_no_outliers_sd_method$bathrooms, use = "complete.obs")
print(paste("Coeficiente de correlación entre price y bathrooms:", cor_bathrooms))

# Calcular el coeficiente de correlación entre price y bedrooms
cor_bedrooms <- cor(model1_no_outliers_sd_method$price, model1_no_outliers_sd_method$bedrooms, use = "complete.obs")
print(paste("Coeficiente de correlación entre price y bedrooms:", cor_bedrooms))

# Calcular el coeficiente de correlación entre price y squareFootage
cor_squareFootage <- cor(model1_no_outliers_sd_method$price, model1_no_outliers_sd_method$squareFootage, use = "complete.obs")
print(paste("Coeficiente de correlación entre price y squareFootage:", cor_squareFootage))

```

# Transformaciones de datos

Las transformaciones de datos, como tomar el logaritmo, la raíz cuadrada o elevar al cuadrado, son técnicas utilizadas en el análisis de datos por diversas razones. El propósito principal de estas transformaciones es modificar la distribución de los datos o la relación entre las variables para satisfacer las suposiciones de los métodos estadísticos o mejorar la interpretabilidad de los resultados. Aquí hay algunas razones comunes para realizar transformaciones:

* **Estabilizar la Varianza:**

En el análisis de regresión, se asume que la varianza de los errores es constante en todos los niveles de las variables predictoras. Si hay una relación no constante entre la varianza y las variables predictoras, las transformaciones como la raíz cuadrada o el logaritmo pueden ayudar a estabilizar la varianza.

* **Linearizar Relaciones No Lineales:**

En algunos casos, la relación entre las variables puede ser no lineal, pero los modelos estadísticos asumen una relación lineal. Tomar el logaritmo o la raíz cuadrada de una variable puede ayudar a linealizar la relación, permitiendo así que se apliquen modelos lineales.

* **Manejar Sesgo en la Distribución:**

Si los datos tienen una distribución sesgada hacia la derecha, donde hay valores atípicos que se encuentran en el extremo superior, tomar el logaritmo puede reducir el sesgo y hacer que la distribución sea más simétrica.

* **Escalamiento:**

Elevar al cuadrado o tomar la raíz cuadrada de una variable puede cambiar la escala de los datos, lo que puede ser útil para resaltar patrones o reducir la importancia de valores extremos.

* **Condiciones de Normalidad:**

En algunos métodos estadísticos, como las pruebas de hipótesis paramétricas, se asume la normalidad de los datos. Transformaciones como el logaritmo pueden ayudar a que los datos se aproximen más a una distribución normal.

* **Mejorar la Interpretabilidad:**

En ocasiones, realizar transformaciones puede hacer que los resultados sean más interpretables o más fácilmente comparables.
Es importante señalar que la elección de una transformación específica depende de la naturaleza de los datos y de los objetivos del análisis. Además, se debe tener precaución al interpretar resultados transformados, ya que pueden afectar la interpretación de los coeficientes o las métricas de evaluación del modelo. La selección de una transformación adecuada a menudo implica explorar y comprender la naturaleza de los datos.


```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Crear un nuevo DataFrame con la transformación logarítmica de squareFootage
m1_log <- model1_no_outliers_sd_method |>
  mutate(squareFootage = log(squareFootage))

# Crear un nuevo DataFrame con la transformación de raíz cuadrada de squareFootage
m1_sqrt <- model1_no_outliers_sd_method |>
  mutate(squareFootage = sqrt(squareFootage))

# Crear un nuevo DataFrame con la transformación cuadrática de squareFootage
m1_sq2 <-  model1_no_outliers_sd_method |>
  mutate(squareFootage = (squareFootage)^2)

# Lista de modelos de regresión lineal con diferentes transformaciones de squareFootage
models <- list(
  "model_no_outliers_sd_method" = lm(price ~ bathrooms + bedrooms + squareFootage + zipCode, data = model1_no_outliers_sd_method),
  "ml_log" = lm(price ~ bathrooms + bedrooms + squareFootage + zipCode, data = m1_log),
  "m1_sqrt"= lm(price ~ bathrooms + bedrooms + squareFootage + zipCode, data = m1_sqrt),
  "m1_sq2" = lm(price ~ bathrooms + bedrooms + squareFootage + zipCode, data = m1_sq2)
)

# Función para crear gráficos de residuos vs valores ajustados
create_residuals_plot <- function(model_name, model) {
  ggplot() +
    geom_point(aes(x = model$fitted.values, y = model$residuals)) +
    labs(
      title = paste("Residuos vs Valores Ajustados para", model_name),
      x = "Valores Ajustados",
      y = "Residuos"
    )
}

# Crear gráficos de residuos vs valores ajustados para cada modelo
plots <- map2(names(models), models, create_residuals_plot)

# Organizar los gráficos en una cuadrícula de 2 columnas
grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], ncol = 2)

```

# Convertir a factores

En R, "convertir a factores" se refiere a cambiar el tipo de datos de una variable a un factor. Un factor es un tipo de dato en R que representa una variable categórica o cualitativa. Las variables categóricas toman un número limitado y fijo de valores que representan categorías o niveles, y no tienen un orden inherente entre ellos.

Cuando conviertes una variable a factor en R, estás indicando que la variable es categórica y que R debe tratarla como tal. Esto es útil cuando trabajas con variables que representan categorías como nombres de regiones, tamaños, colores, etc.

La función básica para convertir una variable a factor en R es **'as.factor()'**

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Imprimir la estructura de cada conjunto de datos en data_sets utilizando str()
map(data_sets, str)
```


```{r, message=FALSE , warning=FALSE, rows.print = 5}

# Definir una función para convertir a factores ciertas columnas en un conjunto de datos
mutate_data_to_fct <- function(df) {
  # Utilizar la función mutate_at para aplicar as.factor a las columnas "zipCode" y "year"
  df <- df |> mutate_at(c("zipCode", "year"), as.factor) 
  return(df)
}

# Aplicar la función mutate_data_to_fct a cada conjunto de datos en data_sets usando map
data_sets <- map(data_sets, mutate_data_to_fct)
# Imprimir la estructura de cada conjunto de datos en data_sets utilizando str()
map(data_sets, str)

```

# Partición de Datos

La partición de datos es una técnica común en estadísticas y aprendizaje automático que implica dividir un conjunto de datos en subconjuntos más pequeños para diversos propósitos, como entrenar modelos, validar modelos y evaluar su rendimiento. Las particiones de datos son fundamentales para garantizar la validez y la generalización de los modelos que se construyen.


```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Definir una función para dividir los datos en conjuntos de entrenamiento y prueba
split_data_fun <- function(df, var1, var2) {
    
    # Obtener los niveles únicos de las variables var1 y var2
    levels1 <- unique (df[[var1]])
    levels2 <- unique (df[[var2]])
    
    # Inicializar índices para conjuntos de entrenamiento y prueba
    training_indices <- c()
    testing_indices  <- c()
    
    # Iterar sobre los niveles de var1 y var2
    for (level1 in levels1) {
        for (level2 in levels2) {
            
            # Filtrar las filas que coinciden con los niveles actuales de var1 y var2
            rows <- which(df[[var1]] == level1 & df[[var2]] == level2)
            
            # Verificar si solo hay una instancia
            if (length(rows) < 2 ) {
                # Si solo hay una instancia, añadirla a los datos de entrenamiento
                training_indices <- c(training_indices, rows)
            } else {
                # Si hay más de una instancia, crear una partición de datos de entrenamiento y prueba
                partition <- createDataPartition(y = rows, p = 0.70, list = FALSE)
                training_indices <- c(training_indices, rows[partition])
                testing_indices <- c(testing_indices, rows[-partition])
            }
        }
    }

    # Devolver una lista con los conjuntos de entrenamiento y prueba
    list(
        training = df[training_indices, ],
        testing = df[testing_indices, ]
    )
}

```

```{r, message=FALSE , warning=FALSE, rows.print = 5}

split_data_frames <- map(data_sets, ~ split_data_fun(.x, "year", "zipCode" ))

```

```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Modelo 1: Regresión Lineal Múltiple con Método de Separación de Datos
# Se utiliza el método de separación de datos para dividir entre entrenamiento y prueba
model <- lm(price ~ year
            + bathrooms
            + bedrooms
            + squareFootage * zipCode,
            data = split_data_frames$model1_no_outliers_sd_method$training) 

# Modelo 2: Regresión Lineal Múltiple con Método de Separación de Datos (IQR)
# Se utiliza el método de separación de datos con el método IQR para manejar valores atípicos
model2 <- lm(price ~ year
              + bathrooms
              + bedrooms
              + squareFootage * zipCode,
              data = split_data_frames$model2_no_outliers_IQR_method$training)

# Modelo 3: Regresión Lineal Múltiple con Método de Separación de Datos (Robusto)
# Se utiliza el método de separación de datos con el método robusto para manejar valores atípicos
model3 <- lm(price ~ year
              + bathrooms
              + bedrooms
              + squareFootage * zipCode,
              data = split_data_frames$model3_no_outliers_robust_method$training)
```


```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Obtener resúmenes y estadísticas adicionales para los modelos

# Resumen del Modelo 1
summary_model  <- summary(model)

# Resumen del Modelo 2
summary_model2 <- summary(model2)

# Resumen del Modelo 3
summary_model3 <- summary(model3)

# Estadísticas adicionales del Modelo 1
glance_model  <- broom::glance(model)

# Estadísticas adicionales del Modelo 2
glance_model2 <- broom::glance(model2)

# Estadísticas adicionales del Modelo 3
glance_model3 <- broom::glance(model3)
```

```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
print(summary_model)
```
```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
glance_model2
```


```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Obtener intervalos de confianza para los coeficientes de los modelos

# Intervalos de confianza del Modelo 1
confint_model  <- confint(model, level = 0.95)

# Intervalos de confianza del Modelo 2
confint_model2 <- confint(model2, level = 0.95)

# Intervalos de confianza del Modelo 3
confint_model3 <- confint(model3, level = 0.95)

```


```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
# Conjunto de datos de prueba para el Modelo 1
m1 <- split_data_frames$model1_no_outliers_sd_method$testing

# Conjunto de datos de prueba para el Modelo 2
m2 <- split_data_frames$model2_no_outliers_IQR_method$testing 

# Conjunto de datos de prueba para el Modelo 3
m3 <- split_data_frames$model3_no_outliers_rubust_method$testing

# Crear una lista para los modelos
models <- list(model = model, model2 = model2, model3 = model3)


class(m3)
```


```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
# # Crear una lista para el conjunto de prueba
# datasets <- list(m1 = m1, m2 = m2, m3 = m3)
# 
# # Función para añadir predicciones a los datos
# add_predictions <- function(dataset, model) {
#     dataset$predicted <- predict(object = model, newdata = dataset)
#     dataset
# }
# 
# # Aplicar la función a cada combinación de conjunto de datos y modelo
# datasets <- mapply(add_predictions, datasets, models, SIMPLIFY = FALSE)
# 
# # Función para crear 3 gráficos de paridad
# create_parity_plot <- function(dataset, name){
#     
#     ggplot(dataset, aes(x = price, y = predicted)) +
#         geom_point() +
#         geom_abline(color = "red", linetype = "dashed") +
#         labs(
#             x = "Precio Observado",
#             y = "Precio Predicho",
#             title = paste("Gráfico de Paridad: Precio Observado vs Predicho (", name, ")"),
#             theme_minimal()
#         )
# }
# 
# # Aplicar la función de creación de gráficos a cada conjunto de datos
# plots <- imap(datasets, ~ create_parity_plot(.x, .y))
# 
# # Organizar los gráficos en una cuadrícula
# grid.arrange(grobs = plots, ncol = 3)
```
```{r}
# Crear una lista para el conjunto de prueba
datasets <- list(m1 = m1, m2 = m2, m3 = m3)

# Función para añadir predicciones a los datos
add_predictions <- function(dataset, model) {
    dataset$predicted <- predict(object = model, newdata = dataset)
    dataset
}

# Aplicar la función a cada combinación de conjunto de datos y modelo
datasets <- mapply(add_predictions, datasets, models, SIMPLIFY = FALSE)

# Función para crear gráficos de paridad
create_parity_plot <- function(dataset, name){
    ggplot(dataset, aes(x = price, y = predicted)) +
        geom_point() +
        geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
        labs(
            x = "Precio Observado",
            y = "Precio Predicho",
            title = paste("Gráfico de Paridad: Precio Observado vs Predicho (", name, ")")
        ) +
        theme_minimal()
}

# Aplicar la función de creación de gráficos a cada conjunto de datos
plots <- imap(datasets, ~ create_parity_plot(.x, .y))

# Imprimir o visualizar los gráficos individualmente para verificar si alguno causa el error

# Organizar los gráficos en una cuadrícula
grid.arrange(grobs = plots, ncol = 3)

```


```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Valores Predichos

calc_metrics <- function(dataset, model) {
    
    predictions <- predict(object = model, newdata = dataset)
    actuals     <- dataset$price # Reemplazar 'price' con el nombre de la variable de respuesta
    
    metrics <- list()
    
    # Cacula tus metricas aqui, luego asignalas a lista
    metrics$Model = deparse(substitute (model))
    metrics$MAE   = mae (actual = actuals, predicted = predictions)
    metrics$MSE   = mse (actual = actuals, predicted = predictions)
    metrics$RMSE  = rmse(actual = actuals, predicted = predictions)
    metrics$MAPE  = mape(actual = actuals, predicted = predictions) 
    return(metrics)
}

results   <- map2(datasets, models, calc_metrics)

metrics_df <-  do.call(rbind, lapply(results, as.data.frame))
```


```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
print(results )
```

```{r, echo=FALSE, message=FALSE , warning=FALSE, rows.print = 5}
print(metrics_df)
```


```{r, message=FALSE , warning=FALSE, rows.print = 5}
# Almacenando los Resultados

model2_metrics <- metrics_df |> slice(2)

# intervalos de confianza

confint_df2  <-  as.data.frame(confint_model2)
confint_df2  <- confint_df2 |> rownames_to_column("Term")
m2_confint_tibble <- as_tibble(confint_df2)

# resultados del modelo

tidy_summary_model2  <- tidy(summary_model2)
model2_f             <- left_join(tidy_summary_model2,
                                  m2_confint_tibble, by = c("term" = "Term"))

write_rds(x = model2_f, file = "01_Informacion_Data/Models_Data/model2_f")
write_rds(x = tidy_summary_model2, file = "01_Informacion_Data/Models_Data/tidy_summary_model2")
```


